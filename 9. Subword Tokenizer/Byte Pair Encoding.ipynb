{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "trained-demonstration",
   "metadata": {},
   "source": [
    "### 들어가기 앞서\n",
    "\n",
    "\n",
    "#### OOV 문제 \n",
    "* 기계가 모르는 단어가 등장하면 그 단어를 단어 집합에 없는 단어란 의미에서 `OOV(Out-Of-Vocabulary)` 또는 `UNK(Unknown Token)` 이라 표현한다. 이와 같이 모르는 단어로 인해 문제를 푸는 것이 까다로워지는 상황을 `OOV 문제` 라고 한다.\n",
    "\n",
    "#### 서브워드 분리(Subword segmenation)\n",
    "* `서브워드 분리` 작업은 하나의 단어는 더 작은 단위의 의미있는 여러 서브워드들의 조합으로 구성된 경우가 많기 때문에 이를 여러 서브워드로 분리해서 단어를 인코딩 및 임베딩하겠다는 의도를 가진 *전처리 작업* \n",
    "* 해당 작업을 통해 OOV 나 희귀 단어, 신조어 같은 문제를 완화시킬 수 있다\n",
    "* 이 책에선 해당 작업을 하는 토크나이저를 `서브워드 토크나이저` 라고 명명."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "textile-apple",
   "metadata": {},
   "source": [
    "# Byte Pair Encoding, BPE\n",
    "\n",
    "* 1994년에 제안된 데이터 압축 알고리즘.\n",
    "* 후에 자연어 처리의 서브워드 분리 알고리즘으로 응용되었다.\n",
    "\n",
    "## 1. 작동 방법\n",
    "* 연속적으로 가장 많이 등장한 글자의 쌍을 찾아서 하나의 글자로 병합하는 방식으로 작동.\n",
    "* 예) `aaabdaaabac` 문자열에 BPE 수행.\n",
    "```python\n",
    "'aaabdaaabac' # 가장 많이 등장하는 글자쌍(byte pair)는 'aa',Z 로 치환\n",
    "'ZabdZabac'   # 'ab' -> 'Y' \n",
    "'ZYdZYac'     # 'ZY' -> 'X'\n",
    "'XdXac'       # 글자쌍 없음. 최종 결과\n",
    "```\n",
    "\n",
    "## 2. 자연어 처리에서의 BPE\n",
    "* 논문 :  https://arxiv.org/pdf/1508.07909.pdf\n",
    "* 글자(character) 단위에서 점차적으로 단어 집합(vocabulary)을 만들어 내는 `Bottom-up` 방식의 접근을 사용한다. \n",
    "* 훈련 데이터에 있는 단어들을 글자 또는 유니코드 단위로 단어 집합 만들고, 가장 많이 등장하는 유니그램을 하나의 유니그램으로 통합.\n",
    "\n",
    "### 2-1 기존 접근\n",
    "* 임의의 훈련 데이터로 얻어낸 임의의 단어와 빈도수.\n",
    "    ```python\n",
    "    # dictionary \n",
    "    'low : 5, lower : 2, newest : 6, widest : 3'`\n",
    "\n",
    "    # vocabulary\n",
    "    'low, lower, newest, widest' \n",
    "\n",
    "    # 'lowest' 가 등장하면 OOV 문제 발생.\n",
    "```\n",
    "\n",
    "### 2-2 BPE 알고리즘 사용한 경우\n",
    "* 앞선 임의의 단어를 글자 단위로 분리.\n",
    "```python\n",
    "    # dictionary\n",
    "    'l o w : 5, l o w e r : 2, n e w e s t : 6, w i d e s t : 3'\n",
    "    \n",
    "    # vocabulary\n",
    "    'l, o, w, e, r, n, w, s, t, i, d' \n",
    "```\n",
    "* BPE의 특징은 알고리즘 동작 횟수를 사용자가 정한다는 점. 여기선 10번 수행한다고 가정.\n",
    "```python\n",
    "    # 총 10회 반복했을 때 결과\n",
    "    # dictionary\n",
    "    'low : 5, low e r : 2, newest : 6, widest : 3'\n",
    "    \n",
    "    # vocabulary\n",
    "    'l, o, w, e, r, n, w, s, t, i, d, es, est, lo, low, ne, new, newest, wi, wid, widest' \n",
    "    \n",
    "    # 'lowest' 가 등장하면 'l o w e s t'로 분리 후 'low','est' 두 단어로 인코딩.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "inappropriate-noise",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원 논문에서 공개한 코드 참고하여 수정한 코드 \n",
    "# 위와 같은 딕셔너리로 진행.\n",
    "dictionary = {'l o w </w>' : 5,\n",
    "              'l o w e r </w>' : 2,\n",
    "              'n e w e s t </w>' : 6,\n",
    "              'w i d e s t </w>' : 3\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sound-exercise",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "# 유니그램의 pair들의 빈도수를 카운트\n",
    "def get_stats(dic) :\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for word, freq in dic.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[symbols[i],symbols[i+1]] += freq\n",
    "    print(\"current freqeunce of pairs : \",dict(pairs))\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fleet-intensity",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# 딕셔너리 합치기\n",
    "def merge_dict(pair, v_in) :\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    for word in v_in :\n",
    "        w_out = p.sub(''.join(pair),word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "starting-valuation",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  1\n",
      "current freqeunce of pairs :  {('l', 'o'): 7, ('o', 'w'): 7, ('w', '</w>'): 5, ('w', 'e'): 8, ('e', 'r'): 2, ('r', '</w>'): 2, ('n', 'e'): 6, ('e', 'w'): 6, ('e', 's'): 9, ('s', 't'): 9, ('t', '</w>'): 9, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'e'): 3}\n",
      "new merge : ('e', 's')\n",
      "dictionary : {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w es t </w>': 6, 'w i d es t </w>': 3}\n",
      "Iteration  2\n",
      "current freqeunce of pairs :  {('l', 'o'): 7, ('o', 'w'): 7, ('w', '</w>'): 5, ('w', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('n', 'e'): 6, ('e', 'w'): 6, ('w', 'es'): 6, ('es', 't'): 9, ('t', '</w>'): 9, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'es'): 3}\n",
      "new merge : ('es', 't')\n",
      "dictionary : {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w est </w>': 6, 'w i d est </w>': 3}\n",
      "Iteration  3\n",
      "current freqeunce of pairs :  {('l', 'o'): 7, ('o', 'w'): 7, ('w', '</w>'): 5, ('w', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('n', 'e'): 6, ('e', 'w'): 6, ('w', 'est'): 6, ('est', '</w>'): 9, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est'): 3}\n",
      "new merge : ('est', '</w>')\n",
      "dictionary : {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\n",
      "Iteration  4\n",
      "current freqeunce of pairs :  {('l', 'o'): 7, ('o', 'w'): 7, ('w', '</w>'): 5, ('w', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('n', 'e'): 6, ('e', 'w'): 6, ('w', 'est</w>'): 6, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est</w>'): 3}\n",
      "new merge : ('l', 'o')\n",
      "dictionary : {'lo w </w>': 5, 'lo w e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\n",
      "Iteration  5\n",
      "current freqeunce of pairs :  {('lo', 'w'): 7, ('w', '</w>'): 5, ('w', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('n', 'e'): 6, ('e', 'w'): 6, ('w', 'est</w>'): 6, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est</w>'): 3}\n",
      "new merge : ('lo', 'w')\n",
      "dictionary : {'low </w>': 5, 'low e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\n",
      "Iteration  6\n",
      "current freqeunce of pairs :  {('low', '</w>'): 5, ('low', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('n', 'e'): 6, ('e', 'w'): 6, ('w', 'est</w>'): 6, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est</w>'): 3}\n",
      "new merge : ('n', 'e')\n",
      "dictionary : {'low </w>': 5, 'low e r </w>': 2, 'ne w est</w>': 6, 'w i d est</w>': 3}\n",
      "Iteration  7\n",
      "current freqeunce of pairs :  {('low', '</w>'): 5, ('low', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('ne', 'w'): 6, ('w', 'est</w>'): 6, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est</w>'): 3}\n",
      "new merge : ('ne', 'w')\n",
      "dictionary : {'low </w>': 5, 'low e r </w>': 2, 'new est</w>': 6, 'w i d est</w>': 3}\n",
      "Iteration  8\n",
      "current freqeunce of pairs :  {('low', '</w>'): 5, ('low', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('new', 'est</w>'): 6, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est</w>'): 3}\n",
      "new merge : ('new', 'est</w>')\n",
      "dictionary : {'low </w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'w i d est</w>': 3}\n",
      "Iteration  9\n",
      "current freqeunce of pairs :  {('low', '</w>'): 5, ('low', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est</w>'): 3}\n",
      "new merge : ('low', '</w>')\n",
      "dictionary : {'low</w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'w i d est</w>': 3}\n",
      "Iteration  10\n",
      "current freqeunce of pairs :  {('low', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est</w>'): 3}\n",
      "new merge : ('w', 'i')\n",
      "dictionary : {'low</w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'wi d est</w>': 3}\n"
     ]
    }
   ],
   "source": [
    "BPE_codes = {}\n",
    "BPE_codes_reverse = {}\n",
    "\n",
    "BPE_counts = 10 \n",
    "\n",
    "for i in range(BPE_counts) :\n",
    "    print(\"Iteration \",i+1)\n",
    "    pairs = get_stats(dictionary)\n",
    "    best = max(pairs,key=pairs.get)\n",
    "    dictionary = merge_dict(best,dictionary)\n",
    "    \n",
    "    BPE_codes[best] = i\n",
    "    BPE_codes_reverse[best[0]+best[1]] = best\n",
    "    \n",
    "    print(\"new merge : {}\".format(best))\n",
    "    print(\"dictionary : {}\".format(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "expired-lottery",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'low</w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'wi d est</w>': 3}\n"
     ]
    }
   ],
   "source": [
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "induced-blocking",
   "metadata": {},
   "source": [
    "### OOV 대처하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "saving-benjamin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split word and make pair\n",
    "def get_pairs(word) :\n",
    "    pairs = set()\n",
    "    prev_char = word[0]\n",
    "    for char in word[1:]:\n",
    "        pairs.add((prev_char,char))\n",
    "        prev_char = char\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "constitutional-wrist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode\n",
    "def encode(origin) :\n",
    "       \n",
    "    word = tuple(origin) + ('</w>',)\n",
    "    print(\"word split into characters : \",word)\n",
    "    \n",
    "    pairs = get_pairs(word)\n",
    "    \n",
    "    if not word :\n",
    "        return origin\n",
    "    \n",
    "    cnt = 0\n",
    "    while True :\n",
    "        cnt += 1\n",
    "        print(\"Iteration \",cnt)\n",
    "        print(\"bigram in word : \",pairs)\n",
    "        bigram = min(pairs,key = lambda pair:BPE_codes.get(pair,float('INF')))\n",
    "        print(\"candidate for merging : \",bigram)\n",
    "        if bigram not in BPE_codes :\n",
    "            print(\"Candidate not in BPE codes, algorithm stop\")\n",
    "            break\n",
    "        \n",
    "        first,second = bigram\n",
    "        new_word = []\n",
    "        i = 0\n",
    "        while i < len(word):\n",
    "            try :\n",
    "                j = word.index(first,i)\n",
    "                new_word.extend(word[i:j])\n",
    "                i = j\n",
    "            except :\n",
    "                new_word.extend(word[i:])\n",
    "                break\n",
    "                \n",
    "            if word[i] == first and i < len(word)-1 and word[i+1] == second :\n",
    "                new_word.append(first+second)\n",
    "                i += 2\n",
    "            else :\n",
    "                new_word.append(word[i])\n",
    "                i += 1\n",
    "        new_word = tuple(new_word)\n",
    "        word = new_word\n",
    "        print(\"word after merging : \",word)\n",
    "        if len(word) == 1:\n",
    "            break\n",
    "        else :\n",
    "            pairs = get_pairs(word)\n",
    "\n",
    "    if word[-1] == '</w>' :\n",
    "        word = word[:-1]\n",
    "    elif word[-1].endswith('</w>'):\n",
    "        word = word[:-1] + (word[-1].replace('</w>',''),)\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "humanitarian-navigator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word split into characters :  ('l', 'o', 'w', 'e', 's', 't', '</w>')\n",
      "Iteration  1\n",
      "bigram in word :  {('w', 'e'), ('e', 's'), ('s', 't'), ('l', 'o'), ('t', '</w>'), ('o', 'w')}\n",
      "candidate for merging :  ('e', 's')\n",
      "word after merging :  ('l', 'o', 'w', 'es', 't', '</w>')\n",
      "Iteration  2\n",
      "bigram in word :  {('l', 'o'), ('w', 'es'), ('es', 't'), ('t', '</w>'), ('o', 'w')}\n",
      "candidate for merging :  ('es', 't')\n",
      "word after merging :  ('l', 'o', 'w', 'est', '</w>')\n",
      "Iteration  3\n",
      "bigram in word :  {('l', 'o'), ('est', '</w>'), ('w', 'est'), ('o', 'w')}\n",
      "candidate for merging :  ('est', '</w>')\n",
      "word after merging :  ('l', 'o', 'w', 'est</w>')\n",
      "Iteration  4\n",
      "bigram in word :  {('l', 'o'), ('w', 'est</w>'), ('o', 'w')}\n",
      "candidate for merging :  ('l', 'o')\n",
      "word after merging :  ('lo', 'w', 'est</w>')\n",
      "Iteration  5\n",
      "bigram in word :  {('lo', 'w'), ('w', 'est</w>')}\n",
      "candidate for merging :  ('lo', 'w')\n",
      "word after merging :  ('low', 'est</w>')\n",
      "Iteration  6\n",
      "bigram in word :  {('low', 'est</w>')}\n",
      "candidate for merging :  ('low', 'est</w>')\n",
      "Candidate not in BPE codes, algorithm stop\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('low', 'est')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode(\"lowest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "catholic-immune",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word split into characters :  ('l', 'o', 'k', 'i', '</w>')\n",
      "Iteration  1\n",
      "bigram in word :  {('l', 'o'), ('i', '</w>'), ('k', 'i'), ('o', 'k')}\n",
      "candidate for merging :  ('l', 'o')\n",
      "word after merging :  ('lo', 'k', 'i', '</w>')\n",
      "Iteration  2\n",
      "bigram in word :  {('i', '</w>'), ('k', 'i'), ('lo', 'k')}\n",
      "candidate for merging :  ('i', '</w>')\n",
      "Candidate not in BPE codes, algorithm stop\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('lo', 'k', 'i')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode(\"loki\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "straight-discipline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word split into characters :  ('h', 'i', 'g', 'h', 'i', 'n', 'g', '</w>')\n",
      "Iteration  1\n",
      "bigram in word :  {('h', 'i'), ('i', 'g'), ('g', 'h'), ('n', 'g'), ('i', 'n'), ('g', '</w>')}\n",
      "candidate for merging :  ('h', 'i')\n",
      "Candidate not in BPE codes, algorithm stop\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('h', 'i', 'g', 'h', 'i', 'n', 'g')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode(\"highing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surrounded-pledge",
   "metadata": {},
   "source": [
    "## 추가적인 알고리즘\n",
    "\n",
    "`그런게 있다 정도로 알고만 넘어감.`\n",
    "\n",
    "### 1. Wordpiece Model\n",
    "\n",
    "* 논문 : https://static.googleusercontent.com/media/research.google.com/ko//pubs/archive/37842.pdf \n",
    "* 구글이 위 Wordpiece Model을 변형하여 번역기에 사용했다는 논문 : https://arxiv.org/pdf/1609.08144.pdf\n",
    "\n",
    "* BPE의 변형 알고리즘. WPM.\n",
    "* BPE 가 빈도수 기반에 가장 많이 등장하는 쌍을 병합하는 것과 달리, 병합되었을 때 코퍼스의 우도(likelihood)를 가장 높이는 쌍을 병합.\n",
    "\n",
    "* 예시\n",
    "\n",
    "    `WPM을 수행하기 이전의 문장: Jet makers feud over seat width with big orders at stake`\n",
    "    \n",
    "    `WPM을 수행한 결과(wordpieces): _J et _makers _fe ud _over _seat _width _with _big _orders _at _stake`\n",
    "    \n",
    "* 언더바(_) 를 붙여 기존의 띄어쓰기인지 구분자 역할의 띄어쓰기 인지 구분. + 복원 가능.\n",
    "* BERT를 훈련하기 위해서 사용되기도 했다.\n",
    "\n",
    "\n",
    "### 2. Unigram Language Model Tokenizer\n",
    "\n",
    "\n",
    "* 논문 : https://arxiv.org/pdf/1804.10959.pdf\n",
    "* 서브워드들에 대해서 손실(loss)을 계산\n",
    "* 손실 : 해당 서브워드가 단어 집합에서 제거되었을 경우, 코퍼스의 우도가 감소하는 정도.\n",
    "* 최악의 영향을 주는 10~20%의 토큰을 제거하며 원하는 단어 집합의 크기가 될 때 까지 반복."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
