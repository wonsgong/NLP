{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "compressed-longitude",
   "metadata": {},
   "source": [
    "# SubwordTextEncoder \n",
    "\n",
    "* 텐서플로우를 통해 사용할 수 있는 서브워드 토크나이저\n",
    "* Wordpiece Model을 채택.\n",
    "* tensorflow_datasets 패키지 사용."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "positive-corpus",
   "metadata": {},
   "source": [
    "## 1. IMDB 리뷰 토큰화하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "isolated-consolidation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        My family and I normally do not watch local mo...\n",
       "1        Believe it or not, this was at one time the wo...\n",
       "2        After some internet surfing, I found the \"Home...\n",
       "3        One of the most unheralded great works of anim...\n",
       "4        It was the Sixties, and anyone with long hair ...\n",
       "                               ...                        \n",
       "49995    the people who came up with this are SICK AND ...\n",
       "49996    The script is so so laughable... this in turn,...\n",
       "49997    \"So there's this bride, you see, and she gets ...\n",
       "49998    Your mind will not be satisfied by this nobud...\n",
       "49999    The chaser's war on everything is a weekly sho...\n",
       "Name: review, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('IMDB_Reviews.csv')\n",
    "train_df['review']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neutral-minutes",
   "metadata": {},
   "source": [
    "`tensorflow_datasets` 의 `deprecated.text.SubwordTextEncoder.build_from_corpus` 인자로 토큰화할 데이터 넣어주면 끝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "sorted-aviation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "                    train_df['review'],target_vocab_size=2**13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "juvenile-racing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size :  8268\n",
      "['the_', ', ', '. ', 'a_', 'and_', 'of_', 'to_', 's_', 'is_', 'br', 'in_', 'I_', 'that_', 'this_', 'it_', ' /><', ' />', 'was_', 'The_', 't_', 'as_', 'with_', 'for_', '.<', 'on_', 'but_', 'movie_', 'are_', ' (', 'have_', 'his_', 'film_', 'not_', 'be_', 'you_', 'ing_', ' \"', 'ed_', 'it', 'd_', 'an_', 'at_', 'by_', 'he_', 'one_', 'who_', 'from_', 'y_', 'or_', 'e_', 'like_', 'all_', '\" ', 'they_', 'so_', 'just_', 'has_', ') ', 'about_', 'her_', 'out_', 'This_', 'some_', 'movie', 'ly_', 'film', 'very_', 'more_', 'It_', 'what_', 'would_', 'when_', 'if_', 'good_', 'up_', 'which_', 'their_', 'only_', 'even_', 'my_', 'really_', 'had_', 'can_', 'no_', 'were_', 'see_', '? ', 'she_', 'than_', '! ', 'there_', 'been_', 'get_', 'into_', 'will_', ' - ', 'much_', 'n_', 'because_', 'ing']\n"
     ]
    }
   ],
   "source": [
    "print(\"vocabulary size : \",tokenizer.vocab_size)\n",
    "print(tokenizer.subwords[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "greatest-cooler",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized sentence :  [137, 8051, 8, 910, 8057, 2169, 36, 7, 103, 13, 14, 32, 18, 79, 681, 8058]\n",
      "origin sentence :  It's mind-blowing to me that this film was even made.\n"
     ]
    }
   ],
   "source": [
    "sent = \"It's mind-blowing to me that this film was even made.\"\n",
    "\n",
    "tokenized_sent = tokenizer.encode(sent)\n",
    "print(\"tokenized sentence : \",tokenized_sent)\n",
    "\n",
    "origin_sent = tokenizer.decode(tokenized_sent)\n",
    "print(\"origin sentence : \",origin_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "pleasant-feelings",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137 ---> It\n",
      "8051 ---> '\n",
      "8 ---> s \n",
      "910 ---> mind\n",
      "8057 ---> -\n",
      "2169 ---> blow\n",
      "36 ---> ing \n",
      "7 ---> to \n",
      "103 ---> me \n",
      "13 ---> that \n",
      "14 ---> this \n",
      "32 ---> film \n",
      "18 ---> was \n",
      "7974 ---> even\n",
      "8132 ---> x\n",
      "8133 ---> y\n",
      "997 ---> z \n",
      "681 ---> made\n",
      "8058 ---> .\n"
     ]
    }
   ],
   "source": [
    "# 임의의 xyz 추가 후 어떻게 매핑되는지 확인\n",
    "\n",
    "sentxyz = \"It's mind-blowing to me that this film was evenxyz made.\"\n",
    "\n",
    "tokenized_sentxyz = tokenizer.encode(sentxyz)\n",
    "\n",
    "for ts in tokenized_sentxyz:\n",
    "    print('{} ---> {}'.format(ts,tokenizer.decode([ts])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "different-hopkins",
   "metadata": {},
   "source": [
    "`xyz` 는 훈련 데이터에서 하나의 단어로 등장한 적이 없기 때문에 각각 전부 분리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neither-message",
   "metadata": {},
   "source": [
    "## 2. Naver 영화 리뷰 토큰화 하기\n",
    "\n",
    "* IMDB 토큰화와 차이점 없기 때문에 생략."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
