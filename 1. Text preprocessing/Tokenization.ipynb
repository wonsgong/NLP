{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "played-synthesis",
   "metadata": {},
   "source": [
    "## 1. Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fancy-spread",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"Starting a home-based restaurant may be an ideal. it doesn't have a food chain or restaurant of their own.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "million-mattress",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "clean-spiritual",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Starting', 'a', 'home-based', 'restaurant', 'may', 'be', 'an', 'ideal', '.', 'it', 'does', \"n't\", 'have', 'a', 'food', 'chain', 'or', 'restaurant', 'of', 'their', 'own', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "varied-smile",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Starting', 'a', 'home', '-', 'based', 'restaurant', 'may', 'be', 'an', 'ideal', '.', 'it', 'doesn', \"'\", 't', 'have', 'a', 'food', 'chain', 'or', 'restaurant', 'of', 'their', 'own', '.']\n"
     ]
    }
   ],
   "source": [
    "print(WordPunctTokenizer().tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "supported-idaho",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['starting', 'a', 'home', 'based', 'restaurant', 'may', 'be', 'an', 'ideal', 'it', \"doesn't\", 'have', 'a', 'food', 'chain', 'or', 'restaurant', 'of', 'their', 'own']\n"
     ]
    }
   ],
   "source": [
    "print(text_to_word_sequence(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finite-operations",
   "metadata": {},
   "source": [
    "### Treebank Tokenization(표준 방법 중 하나)\n",
    "    규칙 1. 하이픈으로 구성된 단어는 하나로 유지한다.\n",
    "    규칙 2. don't 와 같이 \" ' \" 로 접어가 함께 있는 단어는 분리한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "refined-digit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Starting', 'a', 'home-based', 'restaurant', 'may', 'be', 'an', 'ideal.', 'it', 'does', \"n't\", 'have', 'a', 'food', 'chain', 'or', 'restaurant', 'of', 'their', 'own', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "print(tokenizer.tokenize(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executed-healthcare",
   "metadata": {},
   "source": [
    "## 2. Sentence Tokenization "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funded-carry",
   "metadata": {},
   "source": [
    "#### English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cooked-macro",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I am actively looking for Ph.D. students. and you are a Ph.D student.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adequate-second",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I am actively looking for Ph.D. students.', 'and you are a Ph.D student.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sitting-wheat",
   "metadata": {},
   "source": [
    "#### Korean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dense-remark",
   "metadata": {},
   "outputs": [],
   "source": [
    "text='딥 러닝 자연어 처리가 재미있기는 합니다. 그런데 문제는 영어보다 한국어로 할 때 너무 어려워요. 농담아니에요. 이제 해보면 알걸요?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "lined-timothy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['딥 러닝 자연어 처리가 재미있기는 합니다.', '그런데 문제는 영어보다 한국어로 할 때 너무 어려워요.', '농담아니에요.', '이제 해보면 알걸요?']\n"
     ]
    }
   ],
   "source": [
    "from kss import split_sentences\n",
    "\n",
    "print(split_sentences(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consolidated-sessions",
   "metadata": {},
   "source": [
    "## 3. Part-of-speech tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "level-horizontal",
   "metadata": {},
   "source": [
    "#### English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "visible-worst",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I am actively looking for Ph.D. students. and you are a Ph.D student.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "average-popularity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('am', 'VBP'), ('actively', 'RB'), ('looking', 'VBG'), ('for', 'IN'), ('Ph.D.', 'NNP'), ('students.', 'NN'), ('and', 'CC'), ('you', 'PRP'), ('are', 'VBP'), ('a', 'DT'), ('Ph.D', 'NNP'), ('student', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "words = tokenizer.tokenize(text)\n",
    "print(pos_tag(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "possible-donna",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"열심히 코딩한 당신, 연휴에는 여행을 가봐요\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "complimentary-testing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['열심히', '코딩', '한', '당신', ',', '연휴', '에는', '여행', '을', '가봐요']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "print(okt.morphs(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "outstanding-accreditation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('열심히', 'Adverb'), ('코딩', 'Noun'), ('한', 'Josa'), ('당신', 'Noun'), (',', 'Punctuation'), ('연휴', 'Noun'), ('에는', 'Josa'), ('여행', 'Noun'), ('을', 'Josa'), ('가봐요', 'Verb')]\n"
     ]
    }
   ],
   "source": [
    "print(okt.pos(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "satisfied-metabolism",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['코딩', '당신', '연휴', '여행']\n"
     ]
    }
   ],
   "source": [
    "print(okt.nouns(text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
