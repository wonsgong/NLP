{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "typical-wrestling",
   "metadata": {},
   "source": [
    "# Cleaning and Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crucial-planet",
   "metadata": {},
   "source": [
    "    Cleaning(정제) : 코퍼스에서 노이즈 데이터 제거\n",
    "    Normalization(정규화) : 표현 방법이 다른 단어를 통합시켜 같은 단어로 표현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complicated-wilderness",
   "metadata": {},
   "source": [
    "## 1. Normalization\n",
    "\n",
    "### 1) Lemmatization(표제어 추출)\n",
    "    \n",
    "    형태학적 파싱을 통해 단어의 의미를 담고 있는 '어간(stem)' 과 추가적인 의미를 주는 '접사(affix)' 로 분리 후 표제어 추출\n",
    "    \n",
    "    # WordNetLemmatizer 를 사용해 진행. \n",
    "    # 품사 정보를 알고 있을 때 더 좋은 결과나옴.\n",
    "    # 품사 정보 O/X 비교 분석."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "early-feeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "        My fellow citizens: I stand here today humbled by the task before us, \n",
    "        grateful for the trust you've bestowed, mindful of the sacrifices borne by our ancestors. \n",
    "        I thank President Bush forhis service to our nation, as well as the generosity \n",
    "        and cooperation he has shown throughout this transition.\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "native-commissioner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'fellow', 'citizen', ':', 'I', 'stand', 'here', 'today', 'humbled', 'by', 'the', 'task', 'before', 'u', ',', 'grateful', 'for', 'the', 'trust', 'you', \"'ve\", 'bestowed', ',', 'mindful', 'of', 'the', 'sacrifice', 'borne', 'by', 'our', 'ancestors.', 'I', 'thank', 'President', 'Bush', 'forhis', 'service', 'to', 'our', 'nation', ',', 'a', 'well', 'a', 'the', 'generosity', 'and', 'cooperation', 'he', 'ha', 'shown', 'throughout', 'this', 'transition', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "words = tokenizer.tokenize(text)\n",
    "\n",
    "stem = [lemmatizer.lemmatize(word) for word in words]\n",
    "print(stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "developing-danish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'fellow', 'citizen', ':', 'I', 'stand', 'here', 'today', 'humble', 'by', 'the', 'task', 'before', 'u', ',', 'grateful', 'for', 'the', 'trust', 'you', \"'ve\", 'bestow', ',', 'mindful', 'of', 'the', 'sacrifice', 'borne', 'by', 'our', 'ancestors.', 'I', 'thank', 'President', 'Bush', 'forhis', 'service', 'to', 'our', 'nation', ',', 'a', 'well', 'a', 'the', 'generosity', 'and', 'cooperation', 'he', 'have', 'show', 'throughout', 'this', 'transition', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "stem_pos = [lemmatizer.lemmatize(word,get_wordnet_pos(word)) for word in words]\n",
    "print(stem_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "buried-lounge",
   "metadata": {},
   "source": [
    "'bestowed' -> 'bestow' 처럼 보다 정확한 Lemma 출력<br/>\n",
    "추출 후에도 품사 정보를 보존하고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "urban-register",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before :  bestowed  :  [('bestowed', 'VBN')]\n",
      "after :  bestow  :  [('bestow', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "print(\"before : \",words[21],\" : \",pos_tag([words[21]]))\n",
    "print(\"after : \",stem_pos[21],\" : \",pos_tag([stem_pos[21]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "processed-charm",
   "metadata": {},
   "source": [
    "### 2) Stemming(어간 추출)\n",
    "    \n",
    "    정해진 규칙을 가지고 어미를 자름 -> 사전에 없는 단어가 추출 될 수 있다.\n",
    "    \n",
    "    # PorterStemmer 가 영어 어간 추출 시 가장 준수.\n",
    "    # LancasterStemmer 도 지원함.\n",
    "    # 두 추출 방식에 대해서는 홈페이지 참고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "liked-dayton",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'fellow', 'citizen', ':', 'I', 'stand', 'here', 'today', 'humbl', 'by', 'the', 'task', 'befor', 'us', ',', 'grate', 'for', 'the', 'trust', 'you', \"'ve\", 'bestow', ',', 'mind', 'of', 'the', 'sacrific', 'born', 'by', 'our', 'ancestors.', 'I', 'thank', 'presid', 'bush', 'forhi', 'servic', 'to', 'our', 'nation', ',', 'as', 'well', 'as', 'the', 'generos', 'and', 'cooper', 'he', 'ha', 'shown', 'throughout', 'thi', 'transit', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "stem = [stemmer.stem(word) for word in words]\n",
    "print(stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quantitative-hormone",
   "metadata": {},
   "source": [
    "### 3) 표제어 추출과 어간 추출 결과 차이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "reduced-static",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemmatization :  ['be', 'the', 'go', 'have']\n",
      "stemming :  ['am', 'the', 'go', 'ha']\n"
     ]
    }
   ],
   "source": [
    "words = ['am', 'the','going','has']\n",
    "\n",
    "lem = [lemmatizer.lemmatize(word,get_wordnet_pos(word)) for word in words]\n",
    "stm = [stemmer.stem(word) for word in words]\n",
    "\n",
    "print(\"lemmatization : \",lem)\n",
    "print(\"stemming : \",stm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bearing-seeker",
   "metadata": {},
   "source": [
    "### 4) 한글에서 어간 추출\n",
    "\n",
    "    한글은 5언9품사 구조로 이루어져 있다.\n",
    "    그 중 용언에 해당하는 '동사' , '형용사' 는 어간과 어미의 결함으로 구성된다.\n",
    "    \n",
    "#### 활용(conjugation)\n",
    "\n",
    "    용언의 어간이 어미를 가지는 일.\n",
    "    (1) 규칙 활용\n",
    "        어간이 어미를 취할 때, 어간의 모습이 일정할 때.\n",
    "        -> 어미를 단순히 분리해주면 어간 추출이 된다.\n",
    "        예) 잡 + 다. \n",
    "        \n",
    "    (2) 불규칙 활용\n",
    "        어간이 어미를 취할 때, 어간의 모습이 바뀌거나 취하는 어미가 특수한 경우일 때.\n",
    "        -> 좀 더 복잡한 규칙 필요.\n",
    "        예) '노랗-' -> '노랗/노라-' \n",
    "            '푸르+아/어' -> '푸르러' \n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understanding-boston",
   "metadata": {},
   "source": [
    "# Stopword(불용어) \n",
    "\n",
    "\n",
    "    큰 의미가 없는 단어.\n",
    "    nltk 에서 패키지 제공\n",
    "    사용자가 직접 정의할 수도 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "faced-mattress",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords.words('english')[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rapid-opposition",
   "metadata": {},
   "source": [
    "### 불용어 제거하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "molecular-funds",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fellow', 'citizen', ':', 'stand', 'today', 'humble', 'task', 'u', ',', 'grateful', 'trust', \"'ve\", 'bestow', ',', 'mindful', 'sacrifice', 'borne', 'ancestors.', 'thank', 'President', 'Bush', 'forhis', 'service', 'nation', ',', 'well', 'generosity', 'cooperation', 'show', 'throughout', 'transition', '.']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "res = []\n",
    "\n",
    "for word in stem_pos :\n",
    "    if word.lower() not in stop_words :\n",
    "        res.append(word)\n",
    "\n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
