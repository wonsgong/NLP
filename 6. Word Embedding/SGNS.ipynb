{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "helpful-boating",
   "metadata": {},
   "source": [
    "# Skip-Gram with Negative Sampling(SGNS)\n",
    "\n",
    "    기존 학습 과정 : 임베딩 테이블에 있는 모든 단어에 대한 임베딩 벡터 값을 업뎃.\n",
    "    SGNS : 전체 단어 집합이 아닌 일부 단어 집합에만 집중할 수 있는 방법.\n",
    "           중심 단어에 대해서 주변 단어 + 랜덤 단어 -> 단어 집합(1 or 0으로 레이블링)\n",
    "           중심/주변 단어 모두 입력이 되고 두 단어가 실제 윈도우 크기 내에 존재하는 이웃 관계인지 확률을 예측."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affecting-constitution",
   "metadata": {},
   "source": [
    "## 20뉴스 그룹 이용해 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prospective-idaho",
   "metadata": {},
   "source": [
    "### 1) 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "published-request",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total sample :  11314\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "dataset = fetch_20newsgroups(shuffle=True,random_state=1,remove=('headers','footers','quotes'))\n",
    "doc = dataset.data\n",
    "\n",
    "print(\"total sample : \",len(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "opening-coating",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-2ec774ede50a>:5: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  news_df['clean_doc'] = news_df['doc'].str.replace(\"[^a-zA-Z]\",\" \")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "news_df = pd.DataFrame({'doc':doc})\n",
    "\n",
    "news_df['clean_doc'] = news_df['doc'].str.replace(\"[^a-zA-Z]\",\" \")\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x : ' '.join([w for w in x.split() if len(w) > 3]))\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x : x.lower())\n",
    "news_df.replace(\"\",float(\"NaN\"),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "hazardous-region",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "doc          218\n",
       "clean_doc    319\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "waiting-branch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total sample : 10995\n"
     ]
    }
   ],
   "source": [
    "news_df.dropna(inplace=True)\n",
    "print(\"total sample :\",len(news_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "sunrise-acrylic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total samples :  10940\n"
     ]
    }
   ],
   "source": [
    "# 불용어 제거\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "tokenized_doc = news_df['clean_doc'].apply(lambda x : x.split())\n",
    "tokenized_doc = tokenized_doc.apply(lambda x : [word for word in x if word not in stop_words])\n",
    "tokenized_doc = tokenized_doc.to_list()\n",
    "\n",
    "# 단어가 1개 이하인 샘플 제거\n",
    "import numpy as np\n",
    "drop_word = [idx for idx, sent in enumerate(tokenized_doc) if len(sent) <= 1]\n",
    "tokenized_doc = np.delete(tokenized_doc,drop_word,axis=0)\n",
    "\n",
    "print(\"total samples : \",len(tokenized_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "legitimate-respect",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of words set :  64277\n"
     ]
    }
   ],
   "source": [
    "# 정수 인코딩\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(tokenized_doc)\n",
    "\n",
    "word2idx = tokenizer.word_index\n",
    "idx2word = {v:k for k,v in word2idx.items()}\n",
    "encoded = tokenizer.texts_to_sequences(tokenized_doc)\n",
    "\n",
    "vocab_size = len(word2idx) + 1\n",
    "print(\"size of words set : \",vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cathedral-benchmark",
   "metadata": {},
   "source": [
    "### 2) 네거티브 샘플링을 통한 데이터셋 구성\n",
    "    \n",
    "    keras 에서 제공하는 skipgrams 사용.\n",
    "    시간이 많이 걸림으로 상위 10개의 뉴스그룹 샘플에 대해서만 수행."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "proprietary-demand",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
    "\n",
    "# 네거티브 샘플링\n",
    "skip_grams = [skipgrams(sample, vocabulary_size=vocab_size,window_size=10) for sample in encoded]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "constitutional-bahrain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(guilt (4989), rgammon (41237)) -> 0\n",
      "(degree (1530), europeans (4520)) -> 1\n",
      "(occured (4294), marv (30356)) -> 0\n",
      "(power (68), vessels (12350)) -> 0\n",
      "(clearly (661), vpic (3948)) -> 0\n"
     ]
    }
   ],
   "source": [
    "# 첫번째 샘플인 skip_grams[0] 내 데이터셋 확인\n",
    "pairs, labels = skip_grams[0][0], skip_grams[0][1]\n",
    "\n",
    "for i in range(5) :\n",
    "    print(\"({:s} ({:d}), {:s} ({:d})) -> {:d}\".format(\n",
    "          idx2word[pairs[i][0]], pairs[i][0], \n",
    "          idx2word[pairs[i][1]], pairs[i][1], \n",
    "          labels[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neither-audio",
   "metadata": {},
   "source": [
    "### 3) SGNS 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "smoking-bristol",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 1, 100)       6427700     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1, 100)       6427700     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot (Dot)                       (None, 1, 1)         0           embedding[0][0]                  \n",
      "                                                                 embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 1)            0           dot[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 1)            0           reshape[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 12,855,400\n",
      "Trainable params: 12,855,400\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Embedding, Reshape, Activation, Input\n",
    "from tensorflow.keras.layers import Dot\n",
    "\n",
    "\n",
    "embed_size = 100\n",
    "\n",
    "# 중심 단어를 위한 임베딩 테이블\n",
    "w_inputs = Input(shape=(1, ), dtype='int32')\n",
    "word_embedding = Embedding(vocab_size, embed_size)(w_inputs)\n",
    "\n",
    "# 주변 단어를 위한 임베딩 테이블\n",
    "c_inputs = Input(shape=(1, ), dtype='int32')\n",
    "context_embedding  = Embedding(vocab_size, embed_size)(c_inputs)\n",
    "\n",
    "dot_product = Dot(axes=2)([word_embedding, context_embedding])\n",
    "dot_product = Reshape((1,), input_shape=(1, 1))(dot_product)\n",
    "output = Activation('sigmoid')(dot_product)\n",
    "\n",
    "model = Model(inputs=[w_inputs, c_inputs], outputs=output)\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vietnamese-logging",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 커널이 계속 죽네\n",
    "for epoch in range(1, 6):\n",
    "    loss = 0\n",
    "    for _, elem in enumerate(skip_grams):\n",
    "        first_elem = np.array(list(zip(*elem[0]))[0], dtype='int32')\n",
    "        second_elem = np.array(list(zip(*elem[0]))[1], dtype='int32')\n",
    "        labels = np.array(elem[1], dtype='int32')\n",
    "        X = [first_elem, second_elem]\n",
    "        Y = labels\n",
    "        loss += model.train_on_batch(X,Y)  \n",
    "    print('Epoch :',epoch, 'Loss :',loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "given-forwarding",
   "metadata": {},
   "source": [
    "### 4) 결과 확인하기.\n",
    "    \n",
    "\n",
    "학습된 벡터를 txt 로 저장 후 gensim 이용해 로드하면 쉽게 단어 간 유사도 구할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "upper-layout",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('contoller', 0.4101942777633667),\n",
       " ('xtappgeterrordatabase', 0.4040236175060272),\n",
       " ('cradle', 0.40330061316490173),\n",
       " ('xichang', 0.3876398205757141),\n",
       " ('protestants', 0.384991317987442),\n",
       " ('intertestamental', 0.38475894927978516),\n",
       " ('reductions', 0.3805429935455322),\n",
       " ('empt', 0.3667444586753845),\n",
       " ('asai', 0.35873445868492126),\n",
       " ('fisk', 0.3551829159259796)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vector.txt 에 저장\n",
    "f = open('vector.txt','w')\n",
    "f.write('{} {}\\n'.format(vocab_size-1,embed_size))\n",
    "vectors = model.get_weights()[0]\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    f.write('{} {}\\n'.format(word,' '.join(map(str,list(vectors[i,:])))))\n",
    "\n",
    "f.close()\n",
    "\n",
    "# vector.txt 로드\n",
    "\n",
    "import gensim\n",
    "w2v = gensim.models.KeyedVectors.load_word2vec_format('vector.txt',binary=False)\n",
    "\n",
    "w2v.most_similar(positive=['doctor'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "elder-causing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('blinker', 0.39932137727737427),\n",
       " ('snore', 0.3857707381248474),\n",
       " ('comin', 0.3791111707687378),\n",
       " ('marvel', 0.3787321448326111),\n",
       " ('lakshmivarahan', 0.3746788501739502),\n",
       " ('cdfsga', 0.3690260350704193),\n",
       " ('starnet', 0.3647075295448303),\n",
       " ('entranced', 0.3577590882778168),\n",
       " ('pradhaan', 0.356963187456131),\n",
       " ('rainstorm', 0.35132846236228943)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['police'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
