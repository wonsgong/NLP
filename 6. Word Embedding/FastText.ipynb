{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "danish-philip",
   "metadata": {},
   "source": [
    "# FastText\n",
    "\n",
    "    매커니즘 자체는 Word2Vec 의 확장.\n",
    "    차이점 : Word2Vec는 단어를 쪼개질 수 없는 단위로 생각.\n",
    "           FastText는 하나의 단어 안에도 여러 단어들이 존재하는 것으로 간주.\n",
    "           -> 내부 단어(subword)를 고려하여 학습한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "billion-dealer",
   "metadata": {},
   "source": [
    "## 1. 내부 단어(subword) 학습\n",
    "    \n",
    "    각 단어는 글자 단위 n-gram 의 구성을 취급. \n",
    "    시작과 끝을 의미하는 <,> 을 도입하여 벡터로 만들어준다.\n",
    "    기존 단어에 <,>를 붙인 토큰도 추가해준다.\n",
    "    n 은 최소값, 최대값 범위를 설정할 수 있다. 기본값은 최소:3, 최대:6\n",
    "    \n",
    "    ex) n = 3 , apple\n",
    "        <ap, app, ppl, ple, le>, <apple> \n",
    "        -> 단어들을 가지고 Word2Vec을 수행한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neural-absolute",
   "metadata": {},
   "source": [
    "## 2. 모르는 단어(Out Of Vocabulary,OOV)\n",
    "\n",
    "    내부 단어를 통해 OOV 에 대해서도 다른 단어와의 유사도를 계산할 수 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unusual-mortgage",
   "metadata": {},
   "source": [
    "## 3. 단어 집합 내 빈도 수가 적었던 단어(Rare Word)\n",
    "    \n",
    "    단어가 희귀 단어라도, 그 단어의 n-gram이 다른 단어의 n-gram과 겹치는 경우,\n",
    "    비교적 높은 임베딩 벡터값을 가진다. -> 오타가 섞인 단어에 대해서도 일정 수준의 성능을 보인다.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pediatric-promotion",
   "metadata": {},
   "source": [
    "## 4. 실습으로 비교하는 Word2Vec vs FastText\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "distinguished-adapter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리\n",
    "from lxml import etree\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "targetXML = open('ted_en-20160408.xml','r',encoding='UTF8')\n",
    "target_text = etree.parse(targetXML)\n",
    "\n",
    "# xml 파일로부터 <content> </content> 사이 내용 가져오기.\n",
    "parse_text = '\\n'.join(target_text.xpath('//content/text()'))\n",
    "\n",
    "# content 중간 배경음 부분 제거\n",
    "content_text = re.sub(r'\\([^)]*\\)','',parse_text)\n",
    "\n",
    "# 입력 코퍼스에 대해서 nltk 사용 토큰화 후 구두점 제거, 대문자->소문자\n",
    "sent_text = sent_tokenize(content_text)\n",
    "\n",
    "normalized_text = []\n",
    "for string in sent_text :\n",
    "    tokens = re.sub(r\"[^a-z0-9]+\",\" \",string.lower())\n",
    "    normalized_text.append(tokens)\n",
    "\n",
    "    \n",
    "result = [word_tokenize(sentence) for sentence in normalized_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-front",
   "metadata": {},
   "source": [
    "### 1. Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "scheduled-audience",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec.\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "w2v = KeyedVectors.load_word2vec_format('eng_w2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "conditional-steps",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-b20acb32ab81>:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  w2v.wv.most_similar('electrofishing')\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"word 'electrofishing' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-b20acb32ab81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mw2v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'electrofishing'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniforge3/envs/tf/lib/python3.8/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    551\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tf/lib/python3.8/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word 'electrofishing' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "# 없는 단어를 입력하면 에러 발생.\n",
    "w2v.wv.most_similar('electrofishing')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "otherwise-mechanics",
   "metadata": {},
   "source": [
    "### 2. FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "municipal-tennis",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "fasttext = FastText(result,size=100,window=5,min_count=5,workers=4,sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "comparable-bones",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('electrolux', 0.8735015392303467),\n",
       " ('electrolyte', 0.8697566390037537),\n",
       " ('electro', 0.8612461090087891),\n",
       " ('electroshock', 0.8494871854782104),\n",
       " ('electroencephalogram', 0.8427205085754395),\n",
       " ('electrogram', 0.836060643196106),\n",
       " ('electrochemical', 0.8309044241905212),\n",
       " ('electrode', 0.8255390524864197),\n",
       " ('electron', 0.8252688050270081),\n",
       " ('electrons', 0.8225287199020386)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 유사한 단어 계산해서 출력함.\n",
    "fasttext.wv.most_similar('electrofishing')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
