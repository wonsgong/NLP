{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "reduced-shaft",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation(LDA)\n",
    "\n",
    "    LAD 는 문서들은 토픽들의 혼합으로 구성되어져 있으며, \n",
    "    토픽들은 확률 분포에 기반하여 단어들을 생성한다고 가정.\n",
    "    데이터가 주어지면 문서가 생성되던 과정을 역추적한다.\n",
    "    \n",
    "    수행과정 \n",
    "    1. 사용자에게 토픽의 개수 K를 받아온다.\n",
    "    2. 모든 단어를 K개 중 하나의 토픽에 할당한다.\n",
    "    3. 이제 모든 문서의 모든 단어에 대해 아래 사항 반복진행.\n",
    "     3-1. 어떤 문서의 각 단어 w는 자신은 잘못된 토픽에 할당되어져 있지만, 다른 단어들은 전부 올바른\n",
    "          토픽에 할당되어져 있는 상태라고 가정. 단어 w 는 2가지 기준에 따라 토픽이 재할당된다.\n",
    "          P(topic t | document d) :  문서 d의 단어들 중 토픽 t에 해당하는 단어들의 비율\n",
    "          P(word w | topic t) : 각 토픽 t에서 해당 단어 w의 분포.\n",
    "         \n",
    "    LSA와 차이\n",
    "    LSA : DTM 차원 축소해서 축소 차원에서 근접 단어들을 토픽으로 묶는다.\n",
    "    LDA : 단어가 특정 토픽에 존재할 확률과 문서에서 특정 토픽이 존재할 확률을 결합확률로 추정하여 토픽 추출."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "synthetic-phoenix",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20030219</td>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20030219</td>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20030219</td>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20030219</td>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20030219</td>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   publish_date                                      headline_text\n",
       "0      20030219  aba decides against community broadcasting lic...\n",
       "1      20030219     act fire witnesses must be aware of defamation\n",
       "2      20030219     a g calls for infrastructure protection summit\n",
       "3      20030219           air nz staff in aust strike for pay rise\n",
       "4      20030219      air nz strike to affect australian travellers"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"abcnews-date-text.csv\",error_bad_lines=False)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "technological-senior",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = data[['headline_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "crazy-mambo",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       headline_text\n",
       "0  aba decides against community broadcasting lic...\n",
       "1     act fire witnesses must be aware of defamation\n",
       "2     a g calls for infrastructure protection summit\n",
       "3           air nz staff in aust strike for pay rise\n",
       "4      air nz strike to affect australian travellers"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "front-rabbit",
   "metadata": {},
   "source": [
    "텍스트 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "partial-impossible",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-b37d9628502d>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  text['headline_text'] = text.apply(lambda row : nltk.word_tokenize(row['headline_text']),axis=1)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "text['headline_text'] = text.apply(lambda row : nltk.word_tokenize(row['headline_text']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "combined-static",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       headline_text\n",
      "0  [aba, decide, against, community, broadcast, l...\n",
      "1  [act, fire, witness, must, be, aware, of, defa...\n",
      "2  [a, g, call, for, infrastructure, protection, ...\n",
      "3  [air, nz, staff, in, aust, strike, for, pay, r...\n",
      "4  [air, nz, strike, to, affect, australian, trav...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-a1ec0cfcf7fa>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  text['headline_text'] = text['headline_text'].apply(lambda x: [WordNetLemmatizer().lemmatize(word, pos='v') for word in x])\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "text['headline_text'] = text['headline_text'].apply(lambda x: [WordNetLemmatizer().lemmatize(word, pos='v') for word in x])\n",
    "print(text.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "interior-democracy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [decide, against, community, broadcast, licence]\n",
      "1            [fire, witness, must, aware, defamation]\n",
      "2          [call, infrastructure, protection, summit]\n",
      "3                         [staff, aust, strike, rise]\n",
      "4            [strike, affect, australian, travellers]\n",
      "Name: headline_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "tokenized_doc = text['headline_text'].apply(lambda x: [word for word in x if len(word) > 3])\n",
    "print(tokenized_doc[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "average-inspection",
   "metadata": {},
   "source": [
    "TF-IDF 행렬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "artistic-evans",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-2b86ddeb8858>:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  text['headline_text'] = detokenized_doc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1226258, 1000)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#역 토큰화 \n",
    "detokenized_doc = []\n",
    "for i in range(len(text)):\n",
    "    t = \" \".join(tokenized_doc[i])\n",
    "    detokenized_doc.append(t)\n",
    "\n",
    "text['headline_text'] = detokenized_doc\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english',\n",
    "                            max_features=1000)\n",
    "\n",
    "X = vectorizer.fit_transform(text['headline_text'])\n",
    "X.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "placed-associate",
   "metadata": {},
   "source": [
    "토픽 모델링\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "victorian-barcelona",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.00001414e-01 1.00000943e-01 1.00002270e-01 ... 1.00004914e-01\n",
      "  1.00002732e-01 1.00003056e-01]\n",
      " [1.00007711e-01 2.03994281e+02 1.00002099e-01 ... 1.00008282e-01\n",
      "  1.00003643e-01 5.79474578e+02]\n",
      " [1.00001185e-01 1.00000510e-01 1.00001626e-01 ... 1.00015936e-01\n",
      "  1.00008259e-01 1.00008496e-01]\n",
      " ...\n",
      " [1.00002327e-01 1.00000173e-01 6.40483496e+02 ... 1.00011263e-01\n",
      "  1.00002932e-01 1.00006160e-01]\n",
      " [1.00005269e-01 1.00001100e-01 1.00001152e-01 ... 1.00005936e-01\n",
      "  1.00000782e-01 1.00008356e-01]\n",
      " [1.00003439e-01 1.00000202e-01 1.00001393e-01 ... 1.00007561e-01\n",
      "  1.00005423e-01 1.00004653e-01]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "LDA = LatentDirichletAllocation(n_components=10,learning_method='online',\n",
    "                                random_state=777,max_iter=1)\n",
    "\n",
    "LDA.fit_transform(X)\n",
    "print(LDA.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "veterinary-lucas",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: [('court', 8210.31), ('change', 7263.63), ('year', 6107.99), ('woman', 5919.53), ('face', 5696.83)]\n",
      "Topic 2: [('australian', 13282.55), ('donald', 9113.27), ('world', 6872.23), ('shoot', 5313.39), ('leave', 4927.4)]\n",
      "Topic 3: [('coronavirus', 39269.55), ('covid', 19482.21), ('queensland', 12906.92), ('news', 8583.8), ('live', 7907.18)]\n",
      "Topic 4: [('election', 9985.87), ('record', 6380.97), ('crash', 6152.95), ('tasmania', 6141.91), ('make', 6104.51)]\n",
      "Topic 5: [('border', 6378.89), ('state', 6081.07), ('coast', 6014.33), ('restrictions', 5960.47), ('attack', 5827.67)]\n",
      "Topic 6: [('police', 13929.72), ('sydney', 10950.59), ('case', 10135.02), ('government', 9187.84), ('home', 7318.49)]\n",
      "Topic 7: [('australia', 19357.91), ('melbourne', 8899.05), ('report', 5574.88), ('north', 4923.87), ('interview', 4373.91)]\n",
      "Topic 8: [('victoria', 10824.69), ('coronavirus', 8841.6), ('china', 8357.87), ('canberra', 6154.57), ('perth', 4708.38)]\n",
      "Topic 9: [('charge', 8388.17), ('market', 6534.22), ('school', 6214.29), ('years', 5828.01), ('murder', 5688.19)]\n",
      "Topic 10: [('trump', 15893.65), ('help', 5806.85), ('speak', 4845.55), ('indigenous', 4827.19), ('drum', 4286.68)]\n"
     ]
    }
   ],
   "source": [
    "terms = vectorizer.get_feature_names() # 단어 집합. 1,000개의 단어가 저장됨.\n",
    "\n",
    "def get_topics(components, feature_names, n=5):\n",
    "    for idx, topic in enumerate(components):\n",
    "        print(\"Topic %d:\" % (idx+1), [(feature_names[i], topic[i].round(2)) for i in topic.argsort()[:-n - 1:-1]])\n",
    "get_topics(LDA.components_,terms)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
